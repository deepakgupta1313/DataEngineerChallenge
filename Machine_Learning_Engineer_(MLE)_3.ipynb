{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------+------+------+\n",
      "|count_unique_URLs|octet0|octet1|octet2|octet3|\n",
      "+-----------------+------+------+------+------+\n",
      "|               16|    61|    16|   142|   162|\n",
      "|               16|   117|   205|    39|   248|\n",
      "|                2|   117|   203|   181|   144|\n",
      "|               85|   115|   112|   250|   108|\n",
      "|               34|   117|   241|   152|    20|\n",
      "|               16|   202|   174|    92|    10|\n",
      "|               10|   123|   136|   182|   137|\n",
      "|              110|   202|    53|    89|   132|\n",
      "|                6|    27|    63|   186|    72|\n",
      "|              108|    14|   139|    82|   134|\n",
      "|                3|   117|   207|    97|   173|\n",
      "|              112|    27|    34|   244|   251|\n",
      "|               84|   113|   193|   114|    25|\n",
      "|               14|    59|   160|   110|   163|\n",
      "|                7|   120|    61|    47|    36|\n",
      "|                9|   117|   247|   188|    13|\n",
      "|               88|   124|   125|    22|   218|\n",
      "|               37|   103|    42|    88|    34|\n",
      "|               94|   103|    16|    71|     9|\n",
      "|                5|   183|    87|   202|   155|\n",
      "+-----------------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ubuntu/Github/DataEngineerChallenge/spark-2.4.4-bin-hadoop2.7\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, max, sum, mean\n",
    "from pyspark.sql.functions import col, when, count, countDistinct\n",
    "from pyspark.sql.functions import split, concat_ws\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, trim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if \"spark\" not in dir():\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"web_log_analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "logFile = \"data/2015_07_22_mktplace_shop_web_log_sample.log.gz\"\n",
    "numPartitions = 15\n",
    "session_time = 15*60\n",
    "\n",
    "\n",
    "def duration(start, end):\n",
    "    try:\n",
    "        num_of_seconds = (end - start).total_seconds()\n",
    "    except:\n",
    "        num_of_seconds = 0\n",
    "    return num_of_seconds;\n",
    "\n",
    "get_duration = udf(duration, FloatType())\n",
    "\n",
    "def preprocess3(spark):\n",
    "    #print(split(\"1.186.41.1\",\".\").getItem(0).show());\n",
    "    log_schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), False),\n",
    "        StructField(\"elb\", StringType(), False),\n",
    "        StructField(\"client:port\", StringType(), False),\n",
    "        StructField(\"backend:port\", StringType(), False),\n",
    "        StructField(\"request_processing_time\", StringType(), False),\n",
    "        StructField(\"backend_processing_time\", StringType(), False),\n",
    "        StructField(\"response_processing_time\", StringType(), False),\n",
    "        StructField(\"elb_status_code\", StringType(), False),\n",
    "        StructField(\"backend_status_code\", StringType(), False),\n",
    "        StructField(\"received_bytes\", StringType(), False),\n",
    "        StructField(\"sent_bytes\", StringType(), False),\n",
    "        StructField(\"request\", StringType(), False),\n",
    "        StructField(\"user_agent\", StringType(), False),\n",
    "        StructField(\"ssl_cipher\", StringType(), False),\n",
    "        StructField(\"ssl_protocol\", StringType(), False)])\n",
    "    \n",
    "    df = spark.read.csv(logFile, schema=log_schema, sep=\" \").repartition(numPartitions).cache()\n",
    "    split_client = split(df[\"client:port\"], \":\")\n",
    "    split_backend = split(df[\"backend:port\"], \":\")\n",
    "    split_request = split(df[\"request\"], \" \")\n",
    "\n",
    "    df=df.withColumn(\"ip\", split_client.getItem(0)) \\\n",
    "                .withColumn(\"client_port\", split_client.getItem(1)) \\\n",
    "                .withColumn(\"backend_ip\", split_backend.getItem(0)) \\\n",
    "                .withColumn(\"backend_port\", split_backend.getItem(1)) \\\n",
    "                .withColumn(\"request_action\", split_request.getItem(0)) \\\n",
    "                .withColumn(\"request_url\", split_request.getItem(1)) \\\n",
    "                .withColumn(\"request_protocol\", split_request.getItem(2)) \\\n",
    "                .withColumn(\"current_timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                .drop(\"client:port\",\"backend:port\",\"request\").cache()\n",
    "\n",
    "    df=df.select([\"ip\", \"request_url\"]);\n",
    "    \n",
    "    \n",
    "    df=df.na.drop(subset=[\"request_url\"])\n",
    "    df=df.na.drop(subset=[\"ip\"])\n",
    "    \n",
    "    df = df.groupby(\"ip\").agg(countDistinct(\"request_url\").alias(\"count_unique_URLs\"));\n",
    "    df=df.na.drop(subset=[\"count_unique_URLs\"])\n",
    "    \n",
    "    splitt2=split(df[\"ip\"], \"\\\\.\");\n",
    "    df=df.withColumn(\"octet0\", splitt2.getItem(0));\n",
    "    df=df.withColumn(\"octet1\", splitt2.getItem(1));\n",
    "    df=df.withColumn(\"octet2\", splitt2.getItem(2));\n",
    "    df=df.withColumn(\"octet3\", splitt2.getItem(3));\n",
    "    df=df.drop(\"ip\");\n",
    "    df=df.na.drop(subset=[\"octet0\"])\n",
    "    df=df.na.drop(subset=[\"octet1\"])\n",
    "    df=df.na.drop(subset=[\"octet2\"])\n",
    "    df=df.na.drop(subset=[\"octet3\"])\n",
    "    #print(df.dtypes);\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def solve3(spark):\n",
    "    dataset3 = preprocess3(spark).cache()\n",
    "    dataset3.show();\n",
    "    return dataset3.select(\"*\").toPandas();\n",
    "\n",
    "df=solve3(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  octet0 octet1 octet2 octet3  count_unique_URLs\n",
      "0     61     16    142    162                 16\n",
      "1    117    205     39    248                 16\n",
      "2    117    203    181    144                  2\n",
      "3    115    112    250    108                 85\n",
      "4    117    241    152     20                 34\n",
      "(90544, 5)\n"
     ]
    }
   ],
   "source": [
    "df=df[['octet0', 'octet1', 'octet2', 'octet3', 'count_unique_URLs']]\n",
    "\n",
    "print(df.head());\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 81.26309990779191\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df=df.apply(pd.to_numeric) ;\n",
    "\n",
    "X = df[['octet0', 'octet1', 'octet2', 'octet3']]\n",
    "Y =df[['count_unique_URLs']]\n",
    "model = xgboost.XGBRegressor(objective='reg:squarederror')\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "#print(results);\n",
    "print(\"RMSE:\", np.mean(np.sqrt(np.abs(results))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_sparkk)",
   "language": "python",
   "name": "conda_sparkk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
