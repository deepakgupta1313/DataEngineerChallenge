{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+------+------+------+\n",
      "|    session_length|octet0|octet1|octet2|octet3|\n",
      "+------------------+------+------+------+------+\n",
      "| 69.81707191467285|     1|   186|    41|     1|\n",
      "| 231.7906957184896|     1|   186|    76|    11|\n",
      "| 33.04862296581268|     1|   187|   228|   210|\n",
      "| 33.92300724051893|     1|   187|   228|    88|\n",
      "| 59.14387809485197|     1|    23|   101|   102|\n",
      "| 9.247098922729492|     1|    23|   226|    88|\n",
      "| 210.9620418548584|     1|    38|    21|    65|\n",
      "|48.736650466918945|     1|    38|    22|   121|\n",
      "|23.680201530456543|     1|    38|    23|    24|\n",
      "| 7.674998104572296|     1|    39|    14|   229|\n",
      "| 67.07872497009816|     1|    39|    15|   172|\n",
      "|3.8426599502563477|     1|    39|    32|    15|\n",
      "|22.044874668121338|     1|    39|    35|   191|\n",
      "|10.105690002441406|     1|    39|    35|   217|\n",
      "|1.5804920196533203|     1|    39|    46|   200|\n",
      "| 177.6636962890625|     1|    39|    60|   199|\n",
      "|168.64113640785217|     1|    39|    60|    35|\n",
      "| 9.330794131412404|     1|    39|    61|   253|\n",
      "| 4.877890765666962|     1|    39|    62|   102|\n",
      "| 56.01950880885124|     1|    39|    62|   195|\n",
      "+------------------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ubuntu/Github/DataEngineerChallenge/spark-2.4.4-bin-hadoop2.7\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, max, sum, mean\n",
    "from pyspark.sql.functions import col, when, count, countDistinct\n",
    "from pyspark.sql.functions import split, concat_ws\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, trim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if \"spark\" not in dir():\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"web_log_analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "logFile = \"data/2015_07_22_mktplace_shop_web_log_sample.log.gz\"\n",
    "numPartitions = 15\n",
    "session_time = 15*60\n",
    "\n",
    "\n",
    "def duration(start, end):\n",
    "    try:\n",
    "        num_of_seconds = (end - start).total_seconds()\n",
    "    except:\n",
    "        num_of_seconds = 0\n",
    "    return num_of_seconds;\n",
    "\n",
    "get_duration = udf(duration, FloatType())\n",
    "\n",
    "def preprocess2(spark):\n",
    "    #print(split(\"1.186.41.1\",\".\").getItem(0).show());\n",
    "    log_schema = StructType([\n",
    "        StructField(\"timestamp\", StringType(), False),\n",
    "        StructField(\"elb\", StringType(), False),\n",
    "        StructField(\"client:port\", StringType(), False),\n",
    "        StructField(\"backend:port\", StringType(), False),\n",
    "        StructField(\"request_processing_time\", StringType(), False),\n",
    "        StructField(\"backend_processing_time\", StringType(), False),\n",
    "        StructField(\"response_processing_time\", StringType(), False),\n",
    "        StructField(\"elb_status_code\", StringType(), False),\n",
    "        StructField(\"backend_status_code\", StringType(), False),\n",
    "        StructField(\"received_bytes\", StringType(), False),\n",
    "        StructField(\"sent_bytes\", StringType(), False),\n",
    "        StructField(\"request\", StringType(), False),\n",
    "        StructField(\"user_agent\", StringType(), False),\n",
    "        StructField(\"ssl_cipher\", StringType(), False),\n",
    "        StructField(\"ssl_protocol\", StringType(), False)])\n",
    "    \n",
    "    df = spark.read.csv(logFile, schema=log_schema, sep=\" \").repartition(numPartitions).cache()\n",
    "    split_client = split(df[\"client:port\"], \":\")\n",
    "    split_backend = split(df[\"backend:port\"], \":\")\n",
    "    split_request = split(df[\"request\"], \" \")\n",
    "\n",
    "    df=df.withColumn(\"client_ip\", split_client.getItem(0)) \\\n",
    "                .withColumn(\"client_port\", split_client.getItem(1)) \\\n",
    "                .withColumn(\"backend_ip\", split_backend.getItem(0)) \\\n",
    "                .withColumn(\"backend_port\", split_backend.getItem(1)) \\\n",
    "                .withColumn(\"request_action\", split_request.getItem(0)) \\\n",
    "                .withColumn(\"request_url\", split_request.getItem(1)) \\\n",
    "                .withColumn(\"request_protocol\", split_request.getItem(2)) \\\n",
    "                .withColumn(\"current_timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                .drop(\"client:port\",\"backend:port\",\"request\").cache()\n",
    "    \n",
    "    window_func_ip = Window.partitionBy(\"client_ip\").orderBy(\"current_timestamp\")\n",
    "    df = df.withColumn(\"previous_timestamp\",\n",
    "                            lag(col(\"current_timestamp\")).over(window_func_ip)) \\\n",
    "                .withColumn(\"session_duration\",\n",
    "                            get_duration(col(\"previous_timestamp\"), col(\"current_timestamp\"))) \\\n",
    "                .withColumn(\"is_new_session\",\n",
    "                            when((col(\"session_duration\") > session_time), 1).otherwise(0)) \\\n",
    "                .withColumn(\"count_session\",\n",
    "                            sum(col(\"is_new_session\")).over(window_func_ip)) \\\n",
    "                .withColumn(\"ip_session_count\",\n",
    "                            concat_ws(\"_\", col(\"client_ip\"), col(\"count_session\")))\n",
    "\n",
    "    df=df.select([\"ip_session_count\", \"client_ip\", \"request_url\",\n",
    "                               \"previous_timestamp\", \"current_timestamp\",\n",
    "                               \"session_duration\", \"is_new_session\", \"count_session\"]);\n",
    "    \n",
    "    window_func_session = Window.partitionBy(\"ip_session_count\").orderBy(\"current_timestamp\")\n",
    "    df = df.withColumn(\"previous_timestamp_session\",\n",
    "                              lag(df[\"current_timestamp\"]).over(window_func_session)) \\\n",
    "                  .withColumn(\"current_session_duration\",\n",
    "                              get_duration(col(\"previous_timestamp_session\"), col(\"current_timestamp\")))\n",
    "    df = df.groupby(\"ip_session_count\").agg(\n",
    "            sum(\"current_session_duration\").alias(\"session_length\")).cache()\n",
    "    #df = df_session_total.select([mean(\"total_session_time\").alias(\"avg_session_time\")]).cache()\n",
    "    splitt= split(df[\"ip_session_count\"], \"_\");\n",
    "    df=df.withColumn(\"ip\", splitt.getItem(0));\n",
    "    df=df.select([\"ip\", \"session_length\"]);\n",
    "    df=df.na.drop(subset=[\"session_length\"])\n",
    "    df=df.na.drop(subset=[\"ip\"])\n",
    "    #print(df.dtypes);\n",
    "    #df=df.withColumn(\"ip\",trim(col(\"ip\")));\n",
    "    splitt2=split(df[\"ip\"], \"\\\\.\");\n",
    "    #print(splitt2);\n",
    "    df=df.withColumn(\"octet0\", splitt2.getItem(0));\n",
    "    df=df.withColumn(\"octet1\", splitt2.getItem(1));\n",
    "    df=df.withColumn(\"octet2\", splitt2.getItem(2));\n",
    "    df=df.withColumn(\"octet3\", splitt2.getItem(3));\n",
    "    df=df.drop(\"ip\");\n",
    "    df=df.na.drop(subset=[\"octet0\"])\n",
    "    df=df.na.drop(subset=[\"octet1\"])\n",
    "    df=df.na.drop(subset=[\"octet2\"])\n",
    "    df=df.na.drop(subset=[\"octet3\"])\n",
    "    #print(df.dtypes);\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def solve2(spark):\n",
    "    dataset2 = preprocess2(spark).cache()\n",
    "    dataset2.show();\n",
    "    return dataset2.select(\"*\").toPandas();\n",
    "\n",
    "df=solve2(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  octet0 octet1 octet2 octet3  session_length\n",
      "0      1    186     41      1       69.817072\n",
      "1      1    186     76     11      231.790696\n",
      "2      1    187    228    210       33.048623\n",
      "3      1    187    228     88       33.923007\n",
      "4      1     23    101    102       59.143878\n",
      "(88867, 5)\n"
     ]
    }
   ],
   "source": [
    "df=df[['octet0', 'octet1', 'octet2', 'octet3', 'session_length']]\n",
    "print(df.head());\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 248.26853412631772\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df=df.apply(pd.to_numeric) ;\n",
    "\n",
    "X = df[['octet0', 'octet1', 'octet2', 'octet3']]\n",
    "Y =df[['session_length']]\n",
    "model = xgboost.XGBRegressor(objective='reg:squarederror')\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "#print(results);\n",
    "print(\"RMSE:\", np.mean(np.sqrt(np.abs(results))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_sparkk)",
   "language": "python",
   "name": "conda_sparkk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
